{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9f5967ec",
      "metadata": {
        "id": "9f5967ec"
      },
      "source": [
        "# **CS412 Machine Learning - Recitation 4**\n",
        "========================================\n",
        "\n",
        "**Author:** Ece Tosun  \n",
        "**Date:** November 2025\n",
        "\n",
        "**Email**: ece.tosun@sabanciuniv.edu\n",
        "\n",
        "========================================\n",
        "\n",
        "Due to your HW2, logistic regression implementation and after that is removed in this file."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c86f10",
      "metadata": {
        "id": "14c86f10"
      },
      "source": [
        "## **Why Logistic Regression?**\n",
        "Linear regression can predict any real value, so it fails to constrain outputs between 0 and 1. Logistic regression wraps the linear model inside a sigmoid, allowing us to interpret the result as a probability of the positive class. It keeps the decision boundary linear while enabling probabilistic reasoning and principled optimization through maximum likelihood.\n",
        "\n",
        "**We need logistic regression because classification is not a regression problem —> it’s a probability decision problem.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "850f9335",
      "metadata": {},
      "source": [
        "| Aspect | Linear Regression | Logistic Regression |\n",
        "|--------|------------------|-------------------|\n",
        "| Output range | (-∞, +∞) | [0, 1] |\n",
        "| Interpretation | Continuous value | Probability |\n",
        "| Decision boundary | N/A for classification | Linear hyperplane |\n",
        "| Optimization | Least squares | Maximum likelihood |\n",
        "| Use case | Regression | Binary classification |\n",
        "\n",
        "**Bottom line**: Logistic regression keeps the simplicity and interpretability of linear models while making them suitable for classification through the sigmoid transformation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78384bae",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"log_reg_dem.jpg\" width=\"800\">\n",
        "</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "896955ac",
      "metadata": {
        "id": "896955ac"
      },
      "source": [
        "## **Binary Classification Setup**\n",
        "We consider a dataset of $m$ labeled examples $(x^{(i)}, y^{(i)})$ with a single numeric feature: each person's height in centimeters. Labels $y^{(i)} \\in \\{0, 1\\}$ indicate whether the person clears a students threshold (class 1) or not (class 0). Our goal is to learn a weight $w \\in \\mathbb{R}$ and bias $b \\in \\mathbb{R}$ so that tall students are predicted as class 1 with high probability."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb881b3d",
      "metadata": {
        "id": "cb881b3d"
      },
      "source": [
        "## **Gradient Descent Focus**\n",
        "Throughout this recitation we emphasize how different gradient descent strategies behave on the same dataset. We build everything from scratch so that each algebraic step connects directly to the Python implementation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22bb2e14",
      "metadata": {
        "id": "22bb2e14"
      },
      "source": [
        "## **Synthetic Height Dataset**\n",
        "To keep the story concrete we simulate the heights of two groups. Class 0 represents typical students, while class 1 represents students in team sports who are taller on average. The distributions overlap on purpose so that the optimal logistic model cannot perfectly separate the classes -> gradients still need to negotiate noisy labels\n",
        "\n",
        "We standardize the heights to zero mean and unit variance for optimization stability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "228fb2bf",
      "metadata": {
        "id": "228fb2bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, log_loss, classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OXaAyKfFwoWh",
      "metadata": {
        "id": "OXaAyKfFwoWh"
      },
      "source": [
        "## Creating a synthetic dataset of student heights for two distinct groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7d15f8ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7d15f8ba",
        "outputId": "889f220e-db4a-4b39-b798-5f3677027e9b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>height</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>185.175755</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>187.329901</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>171.601409</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>188.530009</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>167.703538</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       height  label\n",
              "0  185.175755      1\n",
              "1  187.329901      1\n",
              "2  171.601409      0\n",
              "3  188.530009      1\n",
              "4  167.703538      0"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rng = np.random.default_rng(42)\n",
        "n_samples = 220\n",
        "class1_ratio = 0.5\n",
        "n_class1 = int(n_samples * class1_ratio)\n",
        "n_class0 = n_samples - n_class1\n",
        "\n",
        "# Generates class with random heights sampled from a normal distribution with an average (loc)\n",
        "# and a standard deviation (scale)\n",
        "heights_class0 = rng.normal(loc=172, scale=5, size=n_class0)  # typical students\n",
        "heights_class1 = rng.normal(loc=182, scale=5, size=n_class1)  # students in team sports\n",
        "\n",
        "heights = np.concatenate([heights_class0, heights_class1])\n",
        "labels = np.concatenate([np.zeros(n_class0, dtype=int), np.ones(n_class1, dtype=int)])\n",
        "\n",
        "df = pd.DataFrame({'height': heights,'label': labels}).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "RSAUpWE1TJqS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSAUpWE1TJqS",
        "outputId": "3dcb5b8b-7679-451f-b55c-01c803cac0d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set size: 154\n",
            "Validation set size: 33\n",
            "Test set size: 33\n"
          ]
        }
      ],
      "source": [
        "X = df[['height']].values\n",
        "y = df['label'].values\n",
        "\n",
        "# train vs temp split (70% train, 30% temp)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split( X, y, test_size=0.30, random_state=42, stratify=y)\n",
        "\n",
        "# temp into validation vs test (50/50 split -> 15% each overall)\n",
        "X_val, X_test, y_val, y_test = train_test_split( X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(\"Train set size:\", len(X_train))\n",
        "print(\"Validation set size:\", len(X_val))\n",
        "print(\"Test set size:\", len(X_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "af6934cf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "af6934cf",
        "outputId": "d36d10c6-e836-4947-f199-42b8c0b38aa9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "fig, ax = plt.subplots(figsize=(7, 4))\n",
        "bins = np.linspace(X.min() - 3, X.max() + 3, 20) # for clarity\n",
        "ax.hist(df.loc[y == 0, 'height'], bins=bins, alpha=0.7, label='Class 0')\n",
        "ax.hist(df.loc[y == 1, 'height'], bins=bins, alpha=0.7, label='Class 1') # alpha for transparency\n",
        "ax.set_xlabel('Height (cm)')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Synthetic Height Dataset')\n",
        "ax.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7021ee50",
      "metadata": {
        "id": "7021ee50"
      },
      "source": [
        "Before learning Logistic Reg. and SGD lest first split our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "UufH9svFVHJW",
      "metadata": {
        "id": "UufH9svFVHJW"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler().fit(X_train) # never fit on X, you would leak information from your test set\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_val = scaler.transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3200533",
      "metadata": {
        "id": "c3200533"
      },
      "source": [
        "## **From Linear Regression to Logistic Regression**\n",
        "Linear regression treats the binary labels as numbers and searches for a straight line that minimizes squared error. That line can wander outside the $[0, 1]$ interval, so its \"probabilities\" are hard to interpret and it makes fragile classification decisions. Logistic regression keeps the same linear score $z = wx + b$ but passes it through the sigmoid, constraining predictions between 0 and 1.\n",
        "\n",
        "1. Linear Regression outputs are unbounded -> bad when we are dealing with probabilistic calculations\n",
        "2. MSE is the wrong loss for classification\n",
        "3. **Classification is inherently non-linear in probability space**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c07befca",
      "metadata": {
        "id": "c07befca"
      },
      "source": [
        "## **Sigmoid Hypothesis**\n",
        "(log odds -> the coefficient normalized by the standard error) \n",
        "\n",
        "Logistic regression models the log-odds of the positive class as a linear function of the inputs:\n",
        "\n",
        "$$\n",
        "z^{(i)} = \\mathbf{w}^\\top \\mathbf{x}^{(i)} + b.\n",
        "$$\n",
        "\n",
        "Applying the logistic sigmoid yields the predicted probability:\n",
        "\n",
        "$$\n",
        "\\hat{y}^{(i)} = \\sigma\\left(z^{(i)}\\right) = \\frac{1}{1 + e^{-z^{(i)}}}.\n",
        "$$\n",
        "\n",
        "Because $\\sigma(z) \\in (0, 1)$ for all $z \\in \\mathbb{R}$, the model outputs well-calibrated probabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c6b273a",
      "metadata": {
        "id": "4c6b273a"
      },
      "source": [
        "### **Sigmoid Properties**\n",
        "- $\\sigma(0) = 0.5$ so the decision boundary occurs where $z = 0$.\n",
        "- The derivative is $\\sigma(z)(1-\\sigma(z))$, which is largest near the boundary and vanishes far away, influencing gradient magnitudes.\n",
        "- The logit function $\\text{logit}(p) = \\log\\frac{p}{1-p}$ is the inverse of the sigmoid, so logistic regression is a linear model in log-odds space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f00fce",
      "metadata": {},
      "source": [
        "<img src=\"sigmoid.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "708adc30",
      "metadata": {
        "id": "708adc30"
      },
      "source": [
        "## **Likelihood and Cross-Entropy Loss**\n",
        "\n",
        "Assuming independent samples, the likelihood of observing the labels given the parameters is\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(\\mathbf{w}, b) = \\prod_{i=1}^{m} \\sigma(z^{(i)})^{y^{(i)}} \\left(1 - \\sigma(z^{(i)})\\right)^{1 - y^{(i)}}.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### From Likelihood to Cross-Entropy Loss (Step-by-Step)\n",
        "\n",
        "We model each binary label using a Bernoulli distribution:\n",
        "\n",
        "$$\n",
        "\\hat{y}^{(i)} = \\sigma(z^{(i)}) = \\sigma(w^T x^{(i)} + b)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "#### Likelihood of observing the data\n",
        "\n",
        "Assuming independent samples:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}(w,b) = \\prod_{i=1}^{m}\n",
        "\\left( \\hat{y}^{(i)} \\right)^{y^{(i)}} \n",
        "\\left( 1 - \\hat{y}^{(i)} \\right)^{\\left(1 - y^{(i)}\\right)}\n",
        "$$\n",
        "\n",
        "A product appears because we assume all samples are **independent**.\n",
        "\n",
        "---\n",
        "\n",
        "#### Take log → Convert product to sum\n",
        "\n",
        "$$\n",
        "\\ell(w,b) \n",
        "= \\log \\mathcal{L}(w,b)\n",
        "= \\sum_{i=1}^{m}\n",
        "\\Big[\n",
        "y^{(i)} \\log \\hat{y}^{(i)} +\n",
        "(1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\n",
        "\\Big]\n",
        "$$\n",
        "\n",
        "> Taking the logarithm simplifies differentiation and improves numerical stability.\n",
        "\n",
        "---\n",
        "\n",
        "#### Convert maximization to minimization\n",
        "\n",
        "Learning ≈ optimization → we minimize a **loss**, not maximize likelihood:\n",
        "\n",
        "$$\n",
        "J(w,b)\n",
        "= -\\frac{1}{m}\\,\\ell(w,b)\n",
        "= -\\frac{1}{m}\\sum_{i=1}^{m}\n",
        "\\Big[\n",
        "y^{(i)} \\log \\hat{y}^{(i)} +\n",
        "(1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\n",
        "\\Big]\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Final Result: Binary Cross-Entropy Loss\n",
        "\n",
        "$$\n",
        "\\boxed{\n",
        "J(w,b)\n",
        "= -\\frac{1}{m}\\sum_{i=1}^{m}\n",
        "\\Big[\n",
        "y^{(i)} \\log \\hat{y}^{(i)} +\n",
        "(1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)})\n",
        "\\Big]\n",
        "}\n",
        "$$\n",
        "\n",
        "This loss:\n",
        "- penalizes **confident wrong predictions** heavily\n",
        "- is **convex** in $w$ and $b$\n",
        "- gives a **probabilistically meaningful** objective"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qH4JzEhmpLjN",
      "metadata": {
        "id": "qH4JzEhmpLjN"
      },
      "source": [
        "## **Logistic Regression Gradient Derivation**\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1 - Loss Function (Single Sample)\n",
        "\n",
        "For binary classification, the loss for one example is:\n",
        "\n",
        "$$\n",
        "\\ell^{(i)} = -\\left[y^{(i)} \\log \\hat{y}^{(i)} + (1-y^{(i)})\\log(1-\\hat{y}^{(i)})\\right]\n",
        "$$\n",
        "\n",
        "**Where:**\n",
        "- $x^{(i)}$ → input\n",
        "- $y^{(i)} \\in \\{0,1\\}$ → true label\n",
        "- $\\hat{y}^{(i)}$ → predicted probability\n",
        "\n",
        "---\n",
        "\n",
        "### Step 2 - Logistic Regression Model (Forward)\n",
        "\n",
        "$$\n",
        "z^{(i)} = w^\\top x^{(i)} + b\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{y}^{(i)} = \\sigma(z^{(i)}) = \\frac{1}{1 + e^{-z^{(i)}}}\n",
        "$$\n",
        "\n",
        "**So the chain is:**\n",
        "\n",
        "$$\n",
        "\\ell^{(i)} \\rightarrow \\hat{y}^{(i)} \\rightarrow z^{(i)} \\rightarrow (w, b)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 3 - Chain Rule Components\n",
        "\n",
        "We need:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell^{(i)}}{\\partial w}, \\quad\n",
        "\\frac{\\partial \\ell^{(i)}}{\\partial b}\n",
        "$$\n",
        "\n",
        "#### A) Loss w.r.t. prediction:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell^{(i)}}{\\partial \\hat{y}^{(i)}}\n",
        "= -\\frac{y^{(i)}}{\\hat{y}^{(i)}} + \\frac{1-y^{(i)}}{1-\\hat{y}^{(i)}}\n",
        "$$\n",
        "\n",
        "#### B) Prediction w.r.t. logit:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\hat{y}^{(i)}}{\\partial z^{(i)}}\n",
        "= \\hat{y}^{(i)}(1 - \\hat{y}^{(i)})\n",
        "$$\n",
        "\n",
        "*(sigmoid derivative makes it a very good binary classification model)*\n",
        "\n",
        "#### C) Logit w.r.t. parameters:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial z^{(i)}}{\\partial w} = x^{(i)}, \\quad\n",
        "\\frac{\\partial z^{(i)}}{\\partial b} = 1\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Step 4 - Combine with Chain Rule\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell^{(i)}}{\\partial z^{(i)}}\n",
        "=\n",
        "\\frac{\\partial \\ell^{(i)}}{\\partial \\hat{y}^{(i)}}\n",
        "\\cdot\n",
        "\\frac{\\partial \\hat{y}^{(i)}}{\\partial z^{(i)}}\n",
        "=\n",
        "\\hat{y}^{(i)} - y^{(i)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Single-Sample Gradients (Used for SGD)\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell^{(i)}}{\\partial w}\n",
        "= (\\hat{y}^{(i)} - y^{(i)}) x^{(i)}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell^{(i)}}{\\partial b}\n",
        "= (\\hat{y}^{(i)} - y^{(i)})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Batch Gradient (All Training Samples, used for Full-Batch)\n",
        "\n",
        "Let:\n",
        "- $m$ = number of samples\n",
        "- $\\hat{y}$ = model predictions vector\n",
        "\n",
        "**Then gradients are:**\n",
        "\n",
        "$$\n",
        "\\nabla_w J\n",
        "= \\frac{1}{m} X^\\top(\\hat{y}-y)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b}\n",
        "= \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}^{(i)} - y^{(i)})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### Gradient Descent Update Rules\n",
        "\n",
        "**These gradients are what gradient descent uses to update the model:**\n",
        "\n",
        "$$\n",
        "w := w - \\eta \\nabla_w J\n",
        "$$\n",
        "\n",
        "$$\n",
        "b := b - \\eta \\frac{\\partial J}{\\partial b}\n",
        "$$\n",
        "\n",
        "where $\\eta$ is the learning rate.\n",
        "\n",
        "---\n",
        "\n",
        "#### Think this term as the error so we can update our weights accordingly:\n",
        "\n",
        "$$\n",
        "(\\hat{y}^{(i)} - y^{(i)})\n",
        "$$\n",
        "\n",
        "The model intuitively says that “If I'm wrong, change weights in the opposite direction of the error.”\n",
        "\n",
        "---\n",
        "\n",
        "This derivation shows why logistic regression is such a fundamental algorithm in machine learning\n",
        "\n",
        "The gradient has a natural interpretation as the prediction error, weighted by the input features."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79113c28",
      "metadata": {
        "id": "79113c28"
      },
      "source": [
        "## **Gradient Descent Algorithms**\n",
        "Let $\\eta$ denote the learning rate. A generic gradient descent update has the form\n",
        "\n",
        "$$\n",
        "\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\, \\nabla_{\\mathbf{w}} J, \\qquad\n",
        "b \\leftarrow b - \\eta \\, \\frac{\\partial J}{\\partial b}.\n",
        "$$\n",
        "\n",
        "- **Batch gradient descent** uses the full dataset for each gradient evaluation. It provides stable steps but can be expensive for large $m$.\n",
        "- **Stochastic gradient descent (SGD)** updates parameters after every single example. It introduces gradient noise that can help escape shallow regions and enables streaming over large datasets. It shuffles the dataset and sample from it one by one."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1203dff9",
      "metadata": {
        "id": "1203dff9"
      },
      "source": [
        "## **Stochastic Sampling Mechanics**\n",
        "Stochastic gradient descent processes one example at a time:\n",
        "1. Start from the list of sample indices $0, \\ldots, m-1$.\n",
        "2. Shuffle the order to create a fresh traversal for the epoch.\n",
        "3. Pick the next index, pull out $(x^{(i)}, y^{(i)})$, compute the gradients from that single example, and update $(w, b)$ immediately.\n",
        "4. Repeat until the epoch ends, then reshuffle for the next pass.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BKuj-T8b_ByP",
      "metadata": {
        "id": "BKuj-T8b_ByP"
      },
      "source": [
        "### Understanding SGD vs Full-Batch Gradient Descent\n",
        "\n",
        "When we set `steps` equal to the total number of training samples,  \n",
        "**SGD completes one full epoch**: it sees every sample once and performs one weight update per sample.\n",
        "\n",
        "However, this is still **not full-batch gradient descent** — the gradient update style is different:\n",
        "\n",
        "- **Full-Batch GD:** Performs **1 update per epoch** using the **average gradient** of *all* samples.  \n",
        "- **SGD:** Performs **many updates per epoch** (one per sample), using **only a single sample’s gradient** each time.\n",
        "\n",
        "So even though both methods have seen the **entire dataset**,  \n",
        "their **learning behavior**, **loss curves**, and **convergence characteristics** are very different.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
